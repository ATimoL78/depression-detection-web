import { useRef, useState, useEffect, useCallback } from "react";
import { Card, CardContent } from "@/components/ui/card";
import { Button } from "@/components/ui/button";
import { Camera, Square, Loader2, AlertCircle, CheckCircle2 } from "lucide-react";
import { toast } from "sonner";
import * as faceapi from "face-api.js";

interface DetectionResult {
  emotion: string;
  confidence: number;
  landmarks: Array<{ x: number; y: number }>;
  expressions: Record<string, number>;
  detectionScore: number;
  auFeatures?: Record<string, number>;
}

interface EnhancedFaceDetectionProps {
  onDetectionResult?: (result: DetectionResult) => void;
}

export default function EnhancedFaceDetection({ onDetectionResult }: EnhancedFaceDetectionProps) {
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const [isDetecting, setIsDetecting] = useState(false);
  const [stream, setStream] = useState<MediaStream | null>(null);
  const [modelLoaded, setModelLoaded] = useState(false);
  const [currentEmotion, setCurrentEmotion] = useState<string>("");
  const [confidence, setConfidence] = useState<number>(0);
  const [fps, setFps] = useState<number>(0);
  const [landmarkCount, setLandmarkCount] = useState<number>(0);
  const [allExpressions, setAllExpressions] = useState<Record<string, number>>({});
  
  const animationRef = useRef<number | null>(null);
  const fpsCounterRef = useRef<{ frames: number; lastTime: number }>({ frames: 0, lastTime: Date.now() });
  const lastFrameTimeRef = useRef<number>(0);

  // åŠ è½½face-api.jsæ¨¡å‹
  useEffect(() => {
    const loadModels = async () => {
      try {
        console.log('ğŸ”§ å¼€å§‹åŠ è½½face-api.jsæ¨¡å‹...');
        const MODEL_URL = "https://cdn.jsdelivr.net/npm/@vladmandic/face-api/model";
        
        await Promise.all([
          faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
          faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
          faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),
          faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),
        ]);
        
        console.log('âœ… face-api.jsæ¨¡å‹åŠ è½½æˆåŠŸ');
        setModelLoaded(true);
        toast.success("AIæƒ…ç»ªè¯†åˆ«æ¨¡å‹åŠ è½½æˆåŠŸ", {
          description: "ç°åœ¨å¯ä»¥å¼€å§‹é¢éƒ¨æ£€æµ‹äº†"
        });
      } catch (error) {
        console.error("âŒ Model loading error:", error);
        toast.error("æ¨¡å‹åŠ è½½å¤±è´¥", {
          description: "è¯·åˆ·æ–°é¡µé¢é‡è¯•"
        });
      }
    };

    loadModels();
  }, []);

  // å¯åŠ¨æ‘„åƒå¤´
  const startCamera = useCallback(async () => {
    if (!modelLoaded) {
      toast.error("è¯·ç­‰å¾…æ¨¡å‹åŠ è½½å®Œæˆ");
      return;
    }

    console.log('ğŸ“· æ­£åœ¨å¯åŠ¨æ‘„åƒå¤´...');
    toast.info("æ­£åœ¨è®¿é—®æ‘„åƒå¤´,è¯·å…è®¸æƒé™...");

    try {
      const mediaStream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 1280 },
          height: { ideal: 720 },
          facingMode: "user",
          frameRate: { ideal: 30 },
        },
      });
      
      console.log('âœ… æ‘„åƒå¤´è®¿é—®æˆåŠŸ');
      
      if (videoRef.current) {
        videoRef.current.srcObject = mediaStream;
        
        // ç­‰å¾…è§†é¢‘å…ƒæ•°æ®åŠ è½½
        await new Promise<void>((resolve) => {
          if (videoRef.current) {
            videoRef.current.onloadedmetadata = () => {
              console.log(`ğŸ¥ è§†é¢‘å·²åŠ è½½: ${videoRef.current?.videoWidth}x${videoRef.current?.videoHeight}`);
              resolve();
            };
          }
        });
        
        await videoRef.current.play();
        console.log('â–¶ï¸ è§†é¢‘å·²å¼€å§‹æ’­æ”¾');
      }
      
      setStream(mediaStream);
      setIsDetecting(true);
      toast.success("æ‘„åƒå¤´å·²å¯åŠ¨", {
        description: "ç²¾å‡†æƒ…ç»ªè¯†åˆ«ä¸­"
      });
    } catch (error: any) {
      console.error("æ‘„åƒå¤´é”™è¯¯:", error);
      
      if (error.name === 'NotAllowedError') {
        toast.error("æ‘„åƒå¤´æƒé™è¢«æ‹’ç»", {
          description: "è¯·å…è®¸æµè§ˆå™¨è®¿é—®æ‘„åƒå¤´"
        });
      } else if (error.name === 'NotFoundError') {
        toast.error("æœªæ‰¾åˆ°æ‘„åƒå¤´è®¾å¤‡", {
          description: "è¯·æ£€æŸ¥æ‘„åƒå¤´æ˜¯å¦è¿æ¥"
        });
      } else if (error.name === 'NotReadableError') {
        toast.error("æ‘„åƒå¤´æ­£è¢«å…¶ä»–åº”ç”¨ä½¿ç”¨", {
          description: "è¯·å…³é—­å…¶ä»–åº”ç”¨"
        });
      } else {
        toast.error(`æ‘„åƒå¤´é”™è¯¯: ${error.message || 'æœªçŸ¥é”™è¯¯'}`);
      }
    }
  }, [modelLoaded]);

  // åœæ­¢æ‘„åƒå¤´
  const stopCamera = useCallback(() => {
    console.log('â¹ï¸ åœæ­¢æ£€æµ‹...');
    
    if (stream) {
      stream.getTracks().forEach(track => {
        track.stop();
        console.log('ğŸ›‘ æ‘„åƒå¤´è½¨é“å·²åœæ­¢');
      });
      setStream(null);
    }
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current);
      animationRef.current = null;
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null;
    }
    
    // æ¸…ç©ºç”»å¸ƒ
    if (canvasRef.current) {
      const ctx = canvasRef.current.getContext('2d');
      if (ctx) {
        ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);
      }
    }
    
    setIsDetecting(false);
    setCurrentEmotion("");
    setConfidence(0);
    setLandmarkCount(0);
    setFps(0);
    setAllExpressions({});
    
    toast.info("æ£€æµ‹å·²åœæ­¢");
  }, [stream]);

  // ç»˜åˆ¶68ä¸ªå…³é”®ç‚¹
  const drawLandmarks = useCallback((
    landmarks: faceapi.FaceLandmarks68,
    canvas: HTMLCanvasElement
  ) => {
    const ctx = canvas.getContext('2d');
    if (!ctx) return;

    const points = landmarks.positions;
    
    // ç»˜åˆ¶æ‰€æœ‰å…³é”®ç‚¹
    ctx.fillStyle = '#00ff00';
    points.forEach((point) => {
      ctx.beginPath();
      ctx.arc(point.x, point.y, 2, 0, 2 * Math.PI);
      ctx.fill();
    });

    // ç»˜åˆ¶è¿æ¥çº¿
    ctx.strokeStyle = '#00ff00';
    ctx.lineWidth = 1;

    // é¢éƒ¨è½®å»“ (0-16)
    ctx.beginPath();
    for (let i = 0; i <= 16; i++) {
      if (i === 0) ctx.moveTo(points[i].x, points[i].y);
      else ctx.lineTo(points[i].x, points[i].y);
    }
    ctx.stroke();

    // å·¦çœ‰æ¯› (17-21)
    ctx.beginPath();
    for (let i = 17; i <= 21; i++) {
      if (i === 17) ctx.moveTo(points[i].x, points[i].y);
      else ctx.lineTo(points[i].x, points[i].y);
    }
    ctx.stroke();

    // å³çœ‰æ¯› (22-26)
    ctx.beginPath();
    for (let i = 22; i <= 26; i++) {
      if (i === 22) ctx.moveTo(points[i].x, points[i].y);
      else ctx.lineTo(points[i].x, points[i].y);
    }
    ctx.stroke();

    // é¼»æ¢ (27-30)
    ctx.beginPath();
    for (let i = 27; i <= 30; i++) {
      if (i === 27) ctx.moveTo(points[i].x, points[i].y);
      else ctx.lineTo(points[i].x, points[i].y);
    }
    ctx.stroke();

    // é¼»åº• (31-35)
    ctx.beginPath();
    for (let i = 31; i <= 35; i++) {
      if (i === 31) ctx.moveTo(points[i].x, points[i].y);
      else ctx.lineTo(points[i].x, points[i].y);
    }
    ctx.closePath();
    ctx.stroke();

    // å·¦çœ¼ (36-41)
    ctx.beginPath();
    for (let i = 36; i <= 41; i++) {
      if (i === 36) ctx.moveTo(points[i].x, points[i].y);
      else ctx.lineTo(points[i].x, points[i].y);
    }
    ctx.closePath();
    ctx.stroke();

    // å³çœ¼ (42-47)
    ctx.beginPath();
    for (let i = 42; i <= 47; i++) {
      if (i === 42) ctx.moveTo(points[i].x, points[i].y);
      else ctx.lineTo(points[i].x, points[i].y);
    }
    ctx.closePath();
    ctx.stroke();

    // å¤–å˜´å”‡ (48-59)
    ctx.beginPath();
    for (let i = 48; i <= 59; i++) {
      if (i === 48) ctx.moveTo(points[i].x, points[i].y);
      else ctx.lineTo(points[i].x, points[i].y);
    }
    ctx.closePath();
    ctx.stroke();

    // å†…å˜´å”‡ (60-67)
    ctx.beginPath();
    for (let i = 60; i <= 67; i++) {
      if (i === 60) ctx.moveTo(points[i].x, points[i].y);
      else ctx.lineTo(points[i].x, points[i].y);
    }
    ctx.closePath();
    ctx.stroke();
  }, []);

  // ç”ŸæˆAUç‰¹å¾(åŸºäºface-api.jsçš„landmarks)
  const generateAUFeatures = useCallback((landmarks: faceapi.FaceLandmarks68): Record<string, number> => {
    const points = landmarks.positions;
    
    // AU1: çœ‰æ¯›å†…ä¾§ä¸Šæ‰¬
    const leftBrowInner = points[21];
    const rightBrowInner = points[22];
    const noseBridge = points[27];
    const browRaise = ((noseBridge.y - leftBrowInner.y) + (noseBridge.y - rightBrowInner.y)) / 2;
    const AU1 = Math.max(0, Math.min(5, browRaise / 5));

    // AU4: çœ‰æ¯›ä¸‹å‹
    const leftBrowOuter = points[17];
    const rightBrowOuter = points[26];
    const leftEyeTop = points[37];
    const rightEyeTop = points[44];
    const browFurrow = ((leftBrowOuter.y - leftEyeTop.y) + (rightBrowOuter.y - rightEyeTop.y)) / 2;
    const AU4 = Math.max(0, Math.min(5, (30 - browFurrow) / 6));

    // AU6: è„¸é¢Šä¸Šæ
    const leftCheek = points[3];
    const rightCheek = points[13];
    const leftMouth = points[48];
    const rightMouth = points[54];
    const cheekRaise = ((leftMouth.y - leftCheek.y) + (rightMouth.y - rightCheek.y)) / 2;
    const AU6 = Math.max(0, Math.min(5, cheekRaise / 20));

    // AU12: å˜´è§’ä¸Šæ‰¬
    const leftMouthCorner = points[48];
    const rightMouthCorner = points[54];
    const mouthCenter = points[51];
    const mouthSmile = ((mouthCenter.y - leftMouthCorner.y) + (mouthCenter.y - rightMouthCorner.y)) / 2;
    const AU12 = Math.max(0, Math.min(5, mouthSmile / 3));

    // AU15: å˜´è§’ä¸‹å‹
    const mouthFrown = ((leftMouthCorner.y - mouthCenter.y) + (rightMouthCorner.y - mouthCenter.y)) / 2;
    const AU15 = Math.max(0, Math.min(5, mouthFrown / 3));

    return {
      AU1: parseFloat(AU1.toFixed(2)),
      AU4: parseFloat(AU4.toFixed(2)),
      AU6: parseFloat(AU6.toFixed(2)),
      AU12: parseFloat(AU12.toFixed(2)),
      AU15: parseFloat(AU15.toFixed(2)),
    };
  }, []);

  // å®æ—¶æ£€æµ‹å¾ªç¯(ä¼˜åŒ–ç‰ˆæœ¬)
  useEffect(() => {
    if (!isDetecting || !videoRef.current || !canvasRef.current || !modelLoaded) return;

    const video = videoRef.current;
    const canvas = canvasRef.current;

    const detectFrame = async () => {
      // å¸§ç‡æ§åˆ¶(æœ€å¤š30fps)
      const currentTime = Date.now();
      const timeSinceLastFrame = currentTime - lastFrameTimeRef.current;
      
      if (timeSinceLastFrame < 33) {
        animationRef.current = requestAnimationFrame(detectFrame);
        return;
      }
      lastFrameTimeRef.current = currentTime;

      if (!video.videoWidth || !video.videoHeight) {
        animationRef.current = requestAnimationFrame(detectFrame);
        return;
      }

      // è®¾ç½®canvaså°ºå¯¸
      if (canvas.width !== video.videoWidth || canvas.height !== video.videoHeight) {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
      }

      try {
        // ä½¿ç”¨face-api.jsè¿›è¡Œæ£€æµ‹
        const detections = await faceapi
          .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
          .withFaceLandmarks()
          .withFaceExpressions();

        if (detections && detections.length > 0) {
          const detection = detections[0];
          
          // æ¸…ç©ºç”»å¸ƒ
          const ctx = canvas.getContext('2d');
          if (ctx) {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
          }

          // ç»˜åˆ¶æ£€æµ‹æ¡†
          const displaySize = { width: video.videoWidth, height: video.videoHeight };
          faceapi.matchDimensions(canvas, displaySize);
          const resizedDetection = faceapi.resizeResults(detection, displaySize);
          
          // ç»˜åˆ¶è¾¹ç•Œæ¡†
          faceapi.draw.drawDetections(canvas, resizedDetection);
          
          // ç»˜åˆ¶68ä¸ªå…³é”®ç‚¹
          drawLandmarks(detection.landmarks, canvas);

          // è·å–æƒ…ç»ª
          const expressions = detection.expressions;
          const sortedExpressions = Object.entries(expressions)
            .sort(([, a], [, b]) => b - a);
          
          const topEmotion = sortedExpressions[0][0];
          const topConfidence = sortedExpressions[0][1];

          setCurrentEmotion(topEmotion);
          setConfidence(Math.round(topConfidence * 100));
          setLandmarkCount(68);
          setAllExpressions(
            Object.fromEntries(
              Object.entries(expressions).map(([key, value]) => [key, Math.round(value * 100)])
            )
          );

          // ç”ŸæˆAUç‰¹å¾
          const auFeatures = generateAUFeatures(detection.landmarks);

          // æ›´æ–°FPS
          fpsCounterRef.current.frames++;
          const now = Date.now();
          const elapsed = now - fpsCounterRef.current.lastTime;
          if (elapsed >= 1000) {
            const currentFps = Math.round((fpsCounterRef.current.frames * 1000) / elapsed);
            setFps(currentFps);
            fpsCounterRef.current.frames = 0;
            fpsCounterRef.current.lastTime = now;
          }

          // å›è°ƒæ£€æµ‹ç»“æœ
          if (onDetectionResult) {
            onDetectionResult({
              emotion: topEmotion,
              confidence: topConfidence,
              landmarks: detection.landmarks.positions.map(p => ({ x: p.x, y: p.y })),
              expressions: expressions,
              detectionScore: detection.detection.score,
              auFeatures,
            });
          }
        } else {
          // æœªæ£€æµ‹åˆ°é¢éƒ¨,æ¸…ç©ºç”»å¸ƒ
          const ctx = canvas.getContext('2d');
          if (ctx) {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
          }
        }
      } catch (error) {
        console.error('æ£€æµ‹é”™è¯¯:', error);
      }

      animationRef.current = requestAnimationFrame(detectFrame);
    };

    detectFrame();

    return () => {
      if (animationRef.current) {
        cancelAnimationFrame(animationRef.current);
      }
    };
  }, [isDetecting, modelLoaded, drawLandmarks, generateAUFeatures, onDetectionResult]);

  return (
    <Card>
      <CardContent className="p-6">
        <div className="relative bg-black rounded-lg overflow-hidden aspect-video">
          {/* è§†é¢‘æµ */}
          <video
            ref={videoRef}
            className="w-full h-full object-cover"
            playsInline
            muted
          />
          
          {/* Canvasè¦†ç›–å±‚ */}
          <canvas
            ref={canvasRef}
            className="absolute inset-0 w-full h-full pointer-events-none"
          />

          {/* çŠ¶æ€æŒ‡ç¤ºå™¨ */}
          <div className="absolute top-4 left-4 space-y-2">
            <div className="flex items-center gap-2 bg-black/70 px-3 py-1.5 rounded-full text-sm">
              {isDetecting ? (
                <>
                  <div className="w-2 h-2 bg-green-500 rounded-full animate-pulse" />
                  <span className="text-white">å®æ—¶æ£€æµ‹ä¸­</span>
                </>
              ) : (
                <>
                  <div className="w-2 h-2 bg-gray-500 rounded-full" />
                  <span className="text-white">ç­‰å¾…å¯åŠ¨</span>
                </>
              )}
            </div>

            {isDetecting && currentEmotion && (
              <div className="bg-black/70 px-3 py-2 rounded-lg text-white text-xs space-y-1">
                <div className="font-semibold">æƒ…ç»ª: {currentEmotion}</div>
                <div>ç½®ä¿¡åº¦: {confidence}%</div>
                <div>å…³é”®ç‚¹: {landmarkCount}</div>
                <div>FPS: {fps}</div>
              </div>
            )}
          </div>

          {/* æƒ…ç»ªåˆ†å¸ƒ */}
          {isDetecting && Object.keys(allExpressions).length > 0 && (
            <div className="absolute bottom-4 right-4 bg-black/70 px-3 py-2 rounded-lg text-white text-xs space-y-1 max-w-xs">
              <div className="font-semibold mb-1">æƒ…ç»ªåˆ†å¸ƒ</div>
              {Object.entries(allExpressions)
                .sort(([, a], [, b]) => b - a)
                .slice(0, 5)
                .map(([emotion, value]) => (
                  <div key={emotion} className="flex justify-between gap-4">
                    <span>{emotion}</span>
                    <span>{value}%</span>
                  </div>
                ))}
            </div>
          )}

          {/* åŠ è½½çŠ¶æ€ */}
          {!modelLoaded && (
            <div className="absolute inset-0 flex items-center justify-center bg-black/50">
              <div className="text-center text-white">
                <Loader2 className="h-8 w-8 animate-spin mx-auto mb-2" />
                <p>æ­£åœ¨åŠ è½½AIæ¨¡å‹...</p>
              </div>
            </div>
          )}
        </div>

        {/* æ§åˆ¶æŒ‰é’® */}
        <div className="mt-4 flex gap-3">
          {!isDetecting ? (
            <Button
              onClick={startCamera}
              disabled={!modelLoaded}
              className="flex-1"
            >
              {modelLoaded ? (
                <>
                  <Camera className="mr-2 h-4 w-4" />
                  å¯åŠ¨æ‘„åƒå¤´
                </>
              ) : (
                <>
                  <Loader2 className="mr-2 h-4 w-4 animate-spin" />
                  åŠ è½½æ¨¡å‹ä¸­...
                </>
              )}
            </Button>
          ) : (
            <Button
              onClick={stopCamera}
              variant="destructive"
              className="flex-1"
            >
              <Square className="mr-2 h-4 w-4" />
              åœæ­¢æ£€æµ‹
            </Button>
          )}
        </div>

        {/* æŠ€æœ¯è¯´æ˜ */}
        <div className="mt-4 bg-muted/50 rounded-lg p-3 text-sm">
          <div className="flex items-start gap-2">
            {modelLoaded ? (
              <>
                <CheckCircle2 className="w-4 h-4 mt-0.5 flex-shrink-0 text-green-500" />
                <span>âœ“ ç³»ç»Ÿå°±ç»ª - ä½¿ç”¨Face-API.jsæ·±åº¦å­¦ä¹ æŠ€æœ¯,æ”¯æŒ68ä¸ªé¢éƒ¨å…³é”®ç‚¹+7ç§ç²¾å‡†æƒ…ç»ªè¯†åˆ«</span>
              </>
            ) : (
              <>
                <Loader2 className="w-4 h-4 mt-0.5 flex-shrink-0 animate-spin" />
                <span>æ­£åœ¨åŠ è½½AIæ¨¡å‹,è¯·ç¨å€™...</span>
              </>
            )}
          </div>
        </div>
      </CardContent>
    </Card>
  );
}
